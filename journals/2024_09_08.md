# 关于 [[Python]] Gemini SDK 的使用
	- Gemini是谷歌的AI大模型，多模态，能以图像、音频作为输入，提供了python SDK和一些文档，学习一下，研究能否集成到自己的工作流中。参考[google-gemini/cookbook](https://github.com/google-gemini/cookbook)。
	- **安装**SDK使用：
	- ```
	  pip install google-generativeai
	  ```
	- **Hello World**，文档建议把key存到环境变量里：
	- ```python
	  import os
	  import google.generativeai as genai
	  
	  # SDK不会直接走系统代理，但会从环境变量里走
	  PROXY_URL = 'http://127.0.0.1:21001'
	  os.environ['http_proxy'] = PROXY_URL 
	  os.environ['HTTP_PROXY'] = PROXY_URL
	  os.environ['https_proxy'] = PROXY_URL
	  os.environ['HTTPS_PROXY'] = PROXY_URL
	  
	  genai.configure(api_key=你的KEY)
	  model = genai.GenerativeModel('models/gemini-1.5-flash') # 获取模型这一步是本地操作，可以随便浪
	  
	  response = model.generate_content("给我一个Python快排实现")
	  print(response.text)
	  ```
	- 大语言模型（LLM）中，输入（即使是非文字数据）和输出都以**token**为计量，token是这样一种粒度，它大于单个unicode字符，但小于一个单词。LLM有一个**上下文窗口**——输入和输出最多能包含多少个token（或者说，LLM同一时间能记住多少token）。Gemini-1.5-flash最多支持1M的上下文窗口。
	- **ChatSession**，上面使用的`model.generate_content`方法是没有记忆的，每次都是开启新对话，如果想让对话保持，可以使用`model.start_chat`方法（创建时可以给定一些初始的对话），这会创建一个session去记录对话。
	- ```python
	  chat = model.start_chat(history=[{'role':'user', 'parts':'Hi my name is Bob'},  {'role':'model', 'parts':'Hi Bob!'}])
	  print(model.count_tokens(chat.history)) # 10 
	  response = chat.send_message('tell me a joke')
	  print(response.text)
	  print(model.count_tokens(chat.history)) # 46
	  response = chat.send_message('tell me another joke')
	  print(response.text)
	  print(model.count_tokens(chat.history)) # 87
	  ```
	- **多模态**，Gemini是多模态的（只考虑图像），图像可以用PIL打开后直接传给Gemini，也可以先用文件API把它上传给谷歌后续再使用，文件只能保存48小时。文件可以是任何类型的文件，包括文本，图像，音频视频等。上传文件时，能提供文件的展示名称以及MIME类型。
	- ```python
	  # 通过文件API上传
	  file = genai.upload_file('img004.jpg')
	  same_file = genai.get_file(file.name) # 文件的唯一标识符是name字段，可以重用
	  res = model.generate_content(['这张图描述了什么', file])
	  
	  # 直接传PIL对象
	  img = Image.open('img004.jpg')
	  res = model.generate_content(['这张图描述了什么', img])
	  ```
	- **JSON模式**，模型，或者某次对话时，可以配置模型使用json模式去进行响应。可以在prompt中，或者**直接通过python类型**去定义返回格式，这样操作好像它有时候会不听，所以如果需要的话还是考虑把返回信息写在prompt里。
	- ```python
	  # 直接整个配置模型返回json
	  model = genai.GenerativeModel('models/gemini-1.5-flash', generation_config={"response_mime_type": "application/json"})
	  res = model.generate_content("""\
	  列举出一些常见的蛋糕类型，你必须使用下面给定的typescript格式作为返回格式，并使用中文回答
	  type Cake = {
	      name: string,
	      description: string
	  }
	  type Response = Cake[]
	  """)
	  
	  # 某次对话时配置返回json，并且指定返回值格式
	  @dataclass
	  class Cake:
	      name: str
	      description: str
	  
	  res = model.generate_content("给出一些常见的蛋糕相关信息，每个字段都必须给定，并使用中文回答", generation_config=genai.GenerationConfig(
	          response_mime_type="application/json",
	          response_schema = list[Cake])) # 注意这里是list不是List！
	  
	  ```
	- **系统指令**，创建模型时可以指定一个额外的（初始的？）上下文去约束它，从这名字看来，它的优先级应该是比较高的。
	- ```python
	  @overload
	  def translate(source_lang: str, target_lang: str, source_text: str) -> str: ...
	  @overload
	  def translate(source_lang: str, target_lang: str, source_text: str, *source_texts: List[str]) -> List[str]: ...
	  def translate(source_lang: str, target_lang: str, source_text: str, *source_texts: List[str]):
	      SYSTEM_INSTRUCTION = f"""
	          你是一个从 {source_lang} 到 {target_lang} 的翻译 agent，在每次对话中，我会给你待翻译的文本列表，你要用json数组形式返回翻译后文本
	      """
	      model = genai.GenerativeModel('models/gemini-1.5-flash', system_instruction=SYSTEM_INSTRUCTION, generation_config={'response_mime_type': 'application/json'})
	      res = model.generate_content([source_text, *source_texts])
	      result: List[str] = json.loads(res.text)
	      if len(source_texts) == 0:
	          return result[0]
	      return result
	  ```
	- DOING **AI Agent（智能体）**，智能体能够根据预定义的目标，策略，环境去**自主地**做出决策和行动，并有能力在过程中调整和学习。Gemini允许用户提供一些函数定义作为上下文，然后作为智能体去响应用户的输入，生成对这些函数进行操作的命令。
	  :LOGBOOK:
	  CLOCK: [2024-09-10 Tue 09:29:44]
	  :END:
	- DOING RAG
	  :LOGBOOK:
	  CLOCK: [2024-09-10 Tue 09:29:50]
	  :END:
	- DOING ReAct
	  :LOGBOOK:
	  CLOCK: [2024-09-10 Tue 09:30:58]
	  :END: